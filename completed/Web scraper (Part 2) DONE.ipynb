{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from more than one page\n",
    "\n",
    "Problem: We scraped our table, but there are a few key pieces of information\n",
    "sitting on each reactor's detail page that we want to include in our\n",
    "analysis.\n",
    "\n",
    "How we're going to deal with it:\n",
    "- Do everything we did before: fetch a page, navigate to the main table and output those details to a CSV\n",
    "- Refine our script so that it dips into the detail page for each reactor\n",
    "- Target the location of these new data with BeautifulSoup\n",
    "- Write a function that will extract them for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add Python's `time` library, which we can use to slow down the speed of our requests to this government website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these [reactor pages](http://www.nrc.gov/info-finder/reactors/bru1.html) for a second. All the data we need actually sits inside a two-column, one-row table near the top â€” we can target that information the same way we did with the main table.\n",
    "\n",
    "There's one issue to solve, though. How do we pull out the individual pieces of information in the absence of HTML tags to latch onto?\n",
    "\n",
    "We can attack it with string functions, splitting it up into a list on line breaks and then searching for certain important words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write our function here. It will take two arguments: A list and a value to find. When it finds a match, it will return the item it matched on. We'll also have it only return the part after the colon (:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finder(a_list, some_value):\n",
    "    for item in a_list:\n",
    "        if some_value.upper() in item.upper():\n",
    "            return item.split(':')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.nrc.gov/reactors/operating/list-power-reactor-units.html'\n",
    "\n",
    "web_page = requests.get(url)\n",
    "soup = BeautifulSoup(web_page.content, 'html.parser')\n",
    "\n",
    "reactor_table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First change to our existing code: We'll send our results to a different file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = open('reactors_more.csv', 'wb')\n",
    "output = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also add the two new fields we'll be grabbing from the detail page to the header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.writerow(['NAME', 'LINK', 'DOCKET', 'LICENSE_NUM', 'TYPE', 'LOCATION', 'OWNER', 'REGION', 'MWT', 'CONTAINMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to revise our loop to include new steps:\n",
    "- Retrieve the individual reactor page, going through the same process as the main table\n",
    "- Isolate the table cell we want to collect\n",
    "- Boil it down to just the text and then turn it into a list based on line breaks\n",
    "- Use a function to search through it and return some new values for a CSV\n",
    "- **PAUSE BETWEEN PAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in reactor_table.find_all('tr')[1:]:\n",
    "    cell = row.find_all('td')\n",
    "    name = cell[0].contents[0].text\n",
    "    link = cell[0].contents[0].get('href')\n",
    "    docket = cell[0].contents[2].strip()\n",
    "    lic_num = cell[1].text\n",
    "    reactype = cell[2].text\n",
    "    location = cell[3].text.encode('utf-8')\n",
    "    owner = cell[4].text.strip().encode('utf-8')\n",
    "    region = cell[5].text\n",
    "\n",
    "    # Add the new steps for this loop below\n",
    "    print('Fetching details for {}...'.format(name))\n",
    "    detail_page = requests.get('http://www.nrc.gov' + link)\n",
    "\n",
    "    detail_soup = BeautifulSoup(detail_page.content, 'html.parser')\n",
    "    \n",
    "    new_data = detail_soup.find_all('td')[1]\n",
    "    data_list = new_data.text.split('\\n')\n",
    "\n",
    "    mwt = finder(data_list, 'licensed mwt')\n",
    "    containment = finder(data_list, 'containment')\n",
    "\n",
    "    output.writerow([name, link, docket, lic_num, reactype, location, owner, region, mwt, containment])\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "csv_file.close()\n",
    "print('All done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
